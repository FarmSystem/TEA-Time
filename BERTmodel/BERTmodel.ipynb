{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 경고 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/content/gdrive/MyDrive/data/\"\n",
    "\n",
    "with open(DATA_PATH + 'TRAINSET.txt', encoding='cp949') as f:\n",
    "    docs = [doc.strip().split('\\t') for doc in f ]\n",
    "    docs = [(doc[0], int(doc[1])) for doc in docs[1:] if len(doc) == 2]\n",
    "    texts, labels = zip(*docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def BERTtokenizer(data, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    for text in data:\n",
    "        tokenized_text = tokenizer.encode_plus(text,\n",
    "                                            max_length=128,\n",
    "                                            add_special_tokens = True,\n",
    "                                            pad_to_max_length=True,\n",
    "                                            return_attention_mask=True,\n",
    "                                              truncation=True)\n",
    "        input_ids.append(tokenized_text['input_ids'])\n",
    "        attention_masks.append(tokenized_text['attention_mask'])\n",
    "        token_type_ids.append(tokenized_text['token_type_ids'])\n",
    "\n",
    "    return input_ids, attention_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 데이터\n",
    "train_input_ids, train_attention_masks, train_token_type_ids = BERTtokenizer(X_train, tokenizer)\n",
    "print(train_input_ids)\n",
    "print(train_attention_masks)\n",
    "print(train_token_type_ids)\n",
    "\n",
    "#테스트 데이터\n",
    "test_input_ids, test_attention_masks, test_token_type_ids = BERTtokenizer(X_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "      }, label\n",
    "\n",
    "\n",
    "def data_encode(input_ids_list, attention_mask_list, token_type_ids_list, label_list):\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
    "\n",
    "BATCH_SIZE=32\n",
    "train_data_encoded = data_encode(train_input_ids, train_attention_masks, train_token_type_ids,y_train).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_data_encoded = data_encode(test_input_ids, test_attention_masks, test_token_type_ids, y_test).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\", \n",
    "    num_labels = 4\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "history = model.fit(train_data_encoded, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=test_data_encoded)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
