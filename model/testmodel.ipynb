{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 크기: \n",
      "TESTCASE.txt                  1.8MB\n",
      "TRAINSET.txt                  110.02MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "DATA_PATH = './data/' #데이터경로 설정\n",
    "print('파일 크기: ')\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    if 'txt' in file:\n",
    "        print(file.ljust(30)+str(round(os.path.getsize(DATA_PATH+ file) / 100000,2))+'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트레인 파일 불러오기\n",
    "train_data = pd.read_csv(DATA_PATH + 'TRAINSET.txt', header = 0, delimiter = '\\t', quoting=3, encoding='cp949')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('학습데이터 전체 개수: {}'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리뷰 전체길이 확인\n",
    "train_length = train_data['발화'].astype(str).apply(len)\n",
    "train_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리뷰 통계 정보\n",
    "print('데이터 길이 최댓값: {}'.format(np.max(train_length)))\n",
    "print('데이터 길이 최솟값: {}'.format(np.min(train_length)))\n",
    "print('데이터 길이 평균값: {:.2f}'.format(np.mean(train_length)))\n",
    "print('데이터 길이 표준편차: {:.2f}'.format(np.std(train_length)))\n",
    "print('데이터 길이 중간값: {}'.format(np.median(train_length)))\n",
    "print('데이터 길이 제1사분위: {}'.format(np.percentile(train_length,25)))\n",
    "print('데이터 길이 제3사분위: {}'.format(np.percentile(train_length,75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 아닌 데이터 모두 제거\n",
    "train_review = [review for review in train_data['발화'] if type(review) is str]\n",
    "train_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불안 0, 분노 1, 기쁨 2\n",
    "\n",
    "print('불안 데이터 갯수: {}'.format(train_data['감정'].value_counts()[0]))\n",
    "print('분노 데이터 갯수: {}'.format(train_data['감정'].value_counts()[1]))\n",
    "print('기쁨 데이터 갯수: {}'.format(train_data['감정'].value_counts()[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    퇴사한 지 얼마 안 됐지만 천천히 직장을 구해보려고.더 좋은 회사가 기다리고 있을지...\n",
       "1    요즘 직장생활이 너무 편하고 좋은 것 같아!우리 회사는 복지가 정말 좋아! 내가 곧...\n",
       "2    취업해야 할 나이인데 취업하고 싶지가 않아.아직 조금 더 놀고 싶은 마음이 커. 인...\n",
       "3    우리 회사는 정말 사내 분위기가 좋아.즐거운 일이 매일 생길 거 같은 기분이야. 내...\n",
       "4    오늘 내가 다니는 회사가 참 좋은 직장이라는 생각이 들었어.회사에서 내가 제안한 프...\n",
       "5    회사에서 전공시험을 봤어. 오늘 시험 결과가 나왔어.열심히 준비한 만큼 원하던 점수...\n",
       "6          오늘 입사 면접을 봤어.면접을 잘 본 것 같아! 그 회사에 취업할 수 있겠어.\n",
       "7    회사에서 나를 참 신뢰하는 것 같아. 그건 기분이 좋아.당연히 내가 업무를 잘 처리...\n",
       "8    먼저 취업한 선배가 면접 비결을 알려줬어. 곧 면접 보러 가는데 든든해.면접 볼 때...\n",
       "9    직장 상사로부터 칭찬을 받았는데 너무 신이 나!다른 사람이 실수한 일을 내가 깔끔하...\n",
       "Name: 발화, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "DATA_PATH = './data/'\n",
    "train_data = pd.read_csv(DATA_PATH + 'TRAINSET.txt', header = 0, delimiter = '\\t', quoting=3, encoding='cp949')\n",
    "\n",
    "train_data['발화'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 함수 만들기\n",
    "def preprocessing(review, okt, remove_stopwords = False, stop_words = []):\n",
    "  #함수인자설명\n",
    "  # review : 전처리할 텍스트\n",
    "  # okt : okt객체를 반복적으로 생성하지 않고 미리 생성 후 인자로 받음\n",
    "  # remove_stopword : 불용어를 제거할지 여부 선택. 기본값 False\n",
    "  # stop_words : 불용어 사전은 사용자가 직접 입력, 기본값 빈 리스트\n",
    "\n",
    "  #1. 한글 및 공백 제외한 문자 모두 제거\n",
    "  review_text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]','',review)\n",
    "  \n",
    "  #2. okt 객체를 활용하여 형태소 단어로 나눔\n",
    "  word_review = okt.morphs(review_text,stem=True)\n",
    "\n",
    "  if remove_stopwords:\n",
    "    #3. 불용어 제거\n",
    "    word_review = [token for token in word_review if not token in stop_words]\n",
    "  return word_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 텍스트 전처리\n",
    "stop_words = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한']\n",
    "okt = Okt()\n",
    "clean_train_review = []\n",
    "\n",
    "for review in train_data['발화']:\n",
    "  # 데이터가 문자열인 경우만 전처리 진행\n",
    "  if type(review) == str:\n",
    "    clean_train_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words= stop_words))\n",
    "  else:\n",
    "    clean_train_review.append([]) # str이 아닌 행은 빈칸으로 놔두기  \n",
    "\n",
    "clean_train_review[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 리뷰도 동일하게 전처리\n",
    "test_data = pd.read_csv(DATA_PATH + 'TESTCASE.txt', header = 0, delimiter='\\t', quoting=3, encoding='cp949')\n",
    "\n",
    "clean_test_review = []\n",
    "for review in test_data['발화']:\n",
    "  if type(review) == str:\n",
    "    clean_test_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))\n",
    "  else:\n",
    "    clean_test_review.append([])\n",
    "    \n",
    "print(clean_test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정\n",
    "\n",
    "# 인덱스 벡터 변환 후 일정 길이 넘어가거나 모자라는 리뷰 패딩처리\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_review)\n",
    "train_sequences = tokenizer.texts_to_sequences(clean_train_review)\n",
    "test_sequences = tokenizer.texts_to_sequences(clean_test_review)\n",
    "\n",
    "word_vocab = tokenizer.word_index # 단어사전형태\n",
    "MAX_SEQUENCE_LENGTH = 8 # 문장 최대 길이\n",
    "\n",
    "# 학습 데이터\n",
    "train_inputs = pad_sequences(train_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "# 학습 데이터 라벨 벡터화\n",
    "train_labels = np.array(train_data['감정'])\n",
    "\n",
    "# 평가 데이터 \n",
    "test_inputs = pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "# 평가 데이터 라벨 벡터화\n",
    "test_label_data = np.array(test_data['감정'])\n",
    "test_labels = to_categorical(test_label_data, num_classes=3) # 수정할 부분 : 출력 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PATH  = './data/' # 경로지정\n",
    "DATA_PATH = 'CLEAN_DATA/' #.npy파일 저장 경로지정\n",
    "TRAIN_INPUT_DATA = 'nsmc_train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'nsmc_train_label.npy'\n",
    "TEST_INPUT_DATA = 'nsmc_test_input.npy'\n",
    "TEST_LABEL_DATA = 'nsmc_test_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "data_configs={}\n",
    "data_configs['vocab'] = word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab) + 1\n",
    "\n",
    "#전처리한 데이터들 파일로저장\n",
    "import os\n",
    "\n",
    "if not os.path.exists(DEFAULT_PATH + DATA_PATH):\n",
    "  os.makedirs(DEFAULT_PATH+DATA_PATH)\n",
    "\n",
    "#전처리 학습데이터 넘파이로 저장\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_INPUT_DATA,'wb'),train_inputs)\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_LABEL_DATA,'wb'),train_labels)\n",
    "#전처리 테스트데이터 넘파이로 저장\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TEST_INPUT_DATA,'wb'),test_inputs)\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TEST_LABEL_DATA,'wb'),test_labels)\n",
    "\n",
    "#데이터 사전 json으로 저장\n",
    "json.dump(data_configs,open(DEFAULT_PATH + DATA_PATH + DATA_CONFIGS,'w'),ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 불러오기\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 전처리 데이터 불러오기\n",
    "DATA_PATH = './data/CLEAN_DATA/'\n",
    "DATA_OUT = './data/DATA_OUT/'\n",
    "INPUT_TRAIN_DATA = 'nsmc_train_input.npy'\n",
    "LABEL_TRAIN_DATA = 'nsmc_train_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "train_input = np.load(open(DATA_PATH + INPUT_TRAIN_DATA,'rb'), allow_pickle=True)\n",
    "train_input = pad_sequences(train_input,maxlen=train_input.shape[1])\n",
    "train_label = np.load(open(DATA_PATH + LABEL_TRAIN_DATA,'rb'), allow_pickle=True)\n",
    "prepro_configs = json.load(open(DATA_PATH+DATA_CONFIGS,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn_classifier_kr'\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 10\n",
    "VALID_SPLIT = 0.1\n",
    "MAX_LEN = train_input.shape[1]\n",
    "\n",
    "kargs={'model_name': model_name, 'vocab_size':prepro_configs['vocab_size'],'embbeding_size':128, 'num_filters':100,'dropout_rate':0.5, 'hidden_dimension':250,'output_dimension':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "class CNNClassifier(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, **kargs):\n",
    "        super(CNNClassifier, self).__init__(name=kargs['model_name'])\n",
    "        self.embedding = layers.Embedding(input_dim=kargs['vocab_size'], output_dim=kargs['embbeding_size'])\n",
    "        self.conv_list = [layers.Conv1D(filters=kargs['num_filters'], kernel_size=kernel_size, padding='valid',activation = tf.keras.activations.relu,\n",
    "                                        kernel_constraint = tf.keras.constraints.MaxNorm(max_value=3)) for kernel_size in [3,4,5]]\n",
    "        self.pooling = layers.GlobalMaxPooling1D()\n",
    "        self.dropout = layers.Dropout(kargs['dropout_rate'])\n",
    "        self.fc1 = layers.Dense(units=kargs['hidden_dimension'],\n",
    "                                activation = tf.keras.activations.relu,\n",
    "                                kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "        self.fc2 = layers.Dense(units=3,  # 수정할 부분 : 출력 차원 변경\n",
    "                                activation=tf.keras.activations.softmax,  # 활성화 함수 변경\n",
    "                                kernel_constraint= tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis = 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DATA_OUT/cnn_classifier_kr -- Folder already exists \n",
      "\n",
      "Epoch 1/10\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.8250 - accuracy: 0.6486\n",
      "Epoch 1: val_accuracy improved from -inf to 0.78478, saving model to ./data/DATA_OUT/cnn_classifier_kr\\weights.h5\n",
      "140/140 [==============================] - 12s 75ms/step - loss: 0.8250 - accuracy: 0.6486 - val_loss: 0.6279 - val_accuracy: 0.7848\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.6492 - accuracy: 0.7383\n",
      "Epoch 2: val_accuracy improved from 0.78478 to 0.82088, saving model to ./data/DATA_OUT/cnn_classifier_kr\\weights.h5\n",
      "140/140 [==============================] - 10s 70ms/step - loss: 0.6492 - accuracy: 0.7383 - val_loss: 0.5885 - val_accuracy: 0.8209\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.7683\n",
      "Epoch 3: val_accuracy did not improve from 0.82088\n",
      "140/140 [==============================] - 10s 70ms/step - loss: 0.5807 - accuracy: 0.7683 - val_loss: 0.6635 - val_accuracy: 0.7742\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5304 - accuracy: 0.7901\n",
      "Epoch 4: val_accuracy did not improve from 0.82088\n",
      "140/140 [==============================] - 10s 69ms/step - loss: 0.5304 - accuracy: 0.7901 - val_loss: 0.6896 - val_accuracy: 0.7683\n",
      "INFO:tensorflow:Assets written to: ./assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./assets\n"
     ]
    }
   ],
   "source": [
    "# 수정\n",
    "\n",
    "from keras.models import save_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = CNNClassifier(**kargs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(name='accuracy')])\n",
    "\n",
    "train_label = to_categorical(train_label, num_classes=3) # 수정할 부분\n",
    "\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
    "checkpoint_path = DATA_OUT + model_name + '\\weights.h5'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "\n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "history = model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
    "\n",
    "save_model(model, './')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step - loss: 0.8767 - accuracy: 0.5832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8766961097717285, 0.5831683278083801]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_TEST_DATA = 'nsmc_test_input.npy'\n",
    "LABEL_TEST_DATA = 'nsmc_test_label.npy'\n",
    "SAVE_FILE_NM = 'weights.h5'\n",
    "\n",
    "test_input = np.load(open(DATA_PATH+INPUT_TEST_DATA,'rb'))\n",
    "test_input = pad_sequences(test_input,maxlen=test_input.shape[1])\n",
    "test_label_data = np.load(open(DATA_PATH + LABEL_TEST_DATA, 'rb'))\n",
    "\n",
    "model.load_weights('./data/DATA_OUT/cnn_classifier_kr/weights.h5')\n",
    "model.evaluate(test_input, test_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "okt = Okt()\n",
    "tokenizer  = Tokenizer()\n",
    "\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "prepro_configs = json.load(open('./data/CLEAN_DATA/'+DATA_CONFIGS,'r'))\n",
    "prepro_configs['vocab'] = word_vocab\n",
    "\n",
    "tokenizer.fit_on_texts(word_vocab)\n",
    "\n",
    "MAX_LENGTH = 8 # 문장최대길이\n",
    "\n",
    "sentence = input('감성분석할 문장을 입력해 주세요.: ')\n",
    "sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\\\s ]','', sentence)\n",
    "stopwords = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한'] # 불용어 추가할 것이 있으면 추가\n",
    "sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "sentence = [word for word in sentence if not word in stopwords] # 불용어 제거\n",
    "vector  = tokenizer.texts_to_sequences(sentence)\n",
    "pad_new = pad_sequences(vector, maxlen = MAX_LENGTH) # 패딩\n",
    "\n",
    "print(sentence)\n",
    "\n",
    "model.load_weights('./data/DATA_OUT/cnn_classifier_kr/weights.h5') # 모델 불러오기\n",
    "predictions = model.predict(pad_new)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 주어진 결과값\n",
    "emotions = np.array(predictions)\n",
    "\n",
    "# 감정 레이블\n",
    "emotion_labels = ['불안', '분노', '기쁨']\n",
    "\n",
    "# 감정 요약 함수\n",
    "def summarize_emotion(emotions, threshold=0.35):\n",
    "    summary = []\n",
    "    for e in emotions:\n",
    "        dominant_emotion = np.argmax(e)  # 가장 높은 점수의 감정 인덱스\n",
    "        if e[dominant_emotion] > threshold:  # 임계값 이상의 점수만을 고려\n",
    "            summary.append(emotion_labels[dominant_emotion])\n",
    "    return summary\n",
    "\n",
    "# 전체적인 기분 요약\n",
    "overall_summary = summarize_emotion(emotions)\n",
    "print(overall_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 실제 감정 데이터와 레이블\n",
    "emotions = np.array(predictions)\n",
    "emotion_labels = ['Anxiety', 'Anger', 'Pleasure']\n",
    "\n",
    "# 실제 요약된 감정 데이터\n",
    "overall_summary = summarize_emotion(emotions)\n",
    "\n",
    "# 요약된 감정 빈도 계산\n",
    "emotion_counts = {emotion: overall_summary.count(emotion) for emotion in emotion_labels}\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(emotion_labels, [emotion_counts[emotion] for emotion in emotion_labels], color='skyblue')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Emotion Graph')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not locate class 'CNNClassifier'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'CNNClassifier', 'config': {'model_name': 'cnn_classifier_kr', 'vocab_size': 23781, 'embbeding_size': 128, 'num_filters': 100, 'dropout_rate': 0.5, 'hidden_dimension': 250, 'output_dimension': 1}, 'registered_name': 'CNNClassifier', 'build_config': {'input_shape': [None, 8]}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'Adam', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'jit_compile': False, 'is_legacy_optimizer': False, 'learning_rate': 0.0010000000474974513, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'keras.losses', 'class_name': 'CategoricalCrossentropy', 'config': {'reduction': 'auto', 'name': 'categorical_crossentropy', 'from_logits': False, 'label_smoothing': 0.0, 'axis': -1, 'fn': 'categorical_crossentropy'}, 'registered_name': None}, 'metrics': [{'module': 'keras.metrics', 'class_name': 'CategoricalAccuracy', 'config': {'name': 'accuracy', 'dtype': 'float32'}, 'registered_name': None}], 'loss_weights': None, 'weighted_metrics': None, 'run_eagerly': None, 'steps_per_execution': None, 'jit_compile': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m# model.load_weights('./data/DATA_OUT/cnn_classifier_kr/weights.h5') # 모델 불러오기\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m model_file:\n\u001b[1;32m---> 35\u001b[0m     model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(model_file)\n\u001b[0;32m     37\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(pad_new)\n\u001b[0;32m     39\u001b[0m \u001b[39m# print(sentence)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m# print(predictions)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m# 주어진 결과값\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MJ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     46\u001b[0m     model \u001b[39m=\u001b[39m saving_lib\u001b[39m.\u001b[39mload_model(filepath, safe_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 48\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     49\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\MJ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\pickle_utils.py:46\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     40\u001b[0m         f\u001b[39m.\u001b[39mwrite(serialized_model)\n\u001b[0;32m     41\u001b[0m     \u001b[39m# When loading, direct import will work for most custom objects\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[39m# though it will require get_config() to be implemented.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[39m# Some custom objects (e.g. an activation in a Dense layer,\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[39m# serialized as a string by Dense.get_config()) will require\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[39m# a custom_object_scope.\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     model \u001b[39m=\u001b[39m saving_lib\u001b[39m.\u001b[39;49mload_model(filepath, safe_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     47\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     48\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\MJ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:275\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    272\u001b[0m             asset_store\u001b[39m.\u001b[39mclose()\n\u001b[0;32m    274\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 275\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\MJ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:240\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39mwith\u001b[39;00m ObjectSharingScope():\n\u001b[1;32m--> 240\u001b[0m     model \u001b[39m=\u001b[39m deserialize_keras_object(\n\u001b[0;32m    241\u001b[0m         config_dict, custom_objects, safe_mode\u001b[39m=\u001b[39;49msafe_mode\n\u001b[0;32m    242\u001b[0m     )\n\u001b[0;32m    244\u001b[0m all_filenames \u001b[39m=\u001b[39m zf\u001b[39m.\u001b[39mnamelist()\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m _VARS_FNAME \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m all_filenames:\n",
      "File \u001b[1;32mc:\\Users\\MJ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:681\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\n\u001b[1;32m--> 681\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m _retrieve_class_or_fn(\n\u001b[0;32m    682\u001b[0m     class_name,\n\u001b[0;32m    683\u001b[0m     registered_name,\n\u001b[0;32m    684\u001b[0m     module,\n\u001b[0;32m    685\u001b[0m     obj_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclass\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    686\u001b[0m     full_config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    687\u001b[0m     custom_objects\u001b[39m=\u001b[39;49mcustom_objects,\n\u001b[0;32m    688\u001b[0m )\n\u001b[0;32m    690\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mcls\u001b[39m, types\u001b[39m.\u001b[39mFunctionType):\n\u001b[0;32m    691\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\MJ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:792\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[1;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    790\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\n\u001b[1;32m--> 792\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    793\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not locate \u001b[39m\u001b[39m{\u001b[39;00mobj_type\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    794\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mMake sure custom classes are decorated with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    795\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull object config: \u001b[39m\u001b[39m{\u001b[39;00mfull_config\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    797\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not locate class 'CNNClassifier'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'CNNClassifier', 'config': {'model_name': 'cnn_classifier_kr', 'vocab_size': 23781, 'embbeding_size': 128, 'num_filters': 100, 'dropout_rate': 0.5, 'hidden_dimension': 250, 'output_dimension': 1}, 'registered_name': 'CNNClassifier', 'build_config': {'input_shape': [None, 8]}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'Adam', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'jit_compile': False, 'is_legacy_optimizer': False, 'learning_rate': 0.0010000000474974513, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'keras.losses', 'class_name': 'CategoricalCrossentropy', 'config': {'reduction': 'auto', 'name': 'categorical_crossentropy', 'from_logits': False, 'label_smoothing': 0.0, 'axis': -1, 'fn': 'categorical_crossentropy'}, 'registered_name': None}, 'metrics': [{'module': 'keras.metrics', 'class_name': 'CategoricalAccuracy', 'config': {'name': 'accuracy', 'dtype': 'float32'}, 'registered_name': None}], 'loss_weights': None, 'weighted_metrics': None, 'run_eagerly': None, 'steps_per_execution': None, 'jit_compile': None}}"
     ]
    }
   ],
   "source": [
    "# 시각화까지\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from konlpy.tag import Okt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "okt = Okt()\n",
    "tokenizer  = Tokenizer()\n",
    "\n",
    "word_vocab = tokenizer.word_index # 수정\n",
    "\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "prepro_configs = json.load(open('./data/CLEAN_DATA/'+DATA_CONFIGS,'r'))\n",
    "prepro_configs['vocab'] = word_vocab\n",
    "\n",
    "tokenizer.fit_on_texts(word_vocab)\n",
    "\n",
    "MAX_LENGTH = 8 # 문장최대길이\n",
    "\n",
    "sentence = input('감성분석할 문장을 입력해 주세요.: ')\n",
    "sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\\\s ]','', sentence)\n",
    "stopwords = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한'] # 불용어 추가할 것이 있으면 추가\n",
    "sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "sentence = [word for word in sentence if not word in stopwords] # 불용어 제거\n",
    "vector  = tokenizer.texts_to_sequences(sentence)\n",
    "pad_new = pad_sequences(vector, maxlen = MAX_LENGTH) # 패딩\n",
    "\n",
    "# model.load_weights('./data/DATA_OUT/cnn_classifier_kr/weights.h5') # 모델 불러오기\n",
    "with open('model.pkl', 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "    \n",
    "predictions = model.predict(pad_new)\n",
    "\n",
    "# print(sentence)\n",
    "# print(predictions)\n",
    "\n",
    "# 주어진 결과값\n",
    "emotions = np.array(predictions)\n",
    "\n",
    "# 감정 레이블\n",
    "emotion_labels = ['불안', '분노', '기쁨']\n",
    "\n",
    "# 감정 요약 함수\n",
    "def summarize_emotion(emotions, threshold=0.35): # 임계값 설정\n",
    "    summary = []\n",
    "    for e in emotions:\n",
    "        dominant_emotion = np.argmax(e)  # 가장 높은 점수의 감정 인덱스\n",
    "        if e[dominant_emotion] > threshold:  # 임계값 이상의 점수만을 고려\n",
    "            summary.append(emotion_labels[dominant_emotion])\n",
    "    return summary\n",
    "\n",
    "# 전체적인 기분 요약\n",
    "overall_summary = summarize_emotion(emotions)\n",
    "# print(overall_summary)\n",
    "\n",
    "# 실제 감정 데이터와 레이블\n",
    "emotions = np.array(predictions)\n",
    "emotion_labels = ['Anxiety', 'Anger', 'Pleasure']\n",
    "\n",
    "# 실제 요약된 감정 데이터\n",
    "overall_summary = summarize_emotion(emotions)\n",
    "\n",
    "# 요약된 감정 빈도 계산\n",
    "emotion_counts = {emotion: overall_summary.count(emotion) for emotion in emotion_labels}\n",
    "print(emotion_counts)\n",
    "\n",
    "# 그래프 그리기\n",
    "'''\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(emotion_labels, [emotion_counts[emotion] for emotion in emotion_labels], color='skyblue')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Emotion Graph')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
