{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "DATA_PATH = 'C:/Users/MJ/Desktop/MyValue/NLP/Model_/data/' #데이터경로 설정\n",
    "print('파일 크기: ')\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    if 'txt' in file:\n",
    "        print(file.ljust(30)+str(round(os.path.getsize(DATA_PATH+ file) / 100000,2))+'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트레인 파일 불러오기\n",
    "train_data = pd.read_csv(DATA_PATH + 'TRAINSET.txt', header = 0, delimiter = '\\t', quoting=3, encoding='cp949')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('학습데이터 전체 개수: {}'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리뷰 전체길이 확인\n",
    "train_length = train_data['발화'].astype(str).apply(len)\n",
    "train_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리뷰 통계 정보\n",
    "print('데이터 길이 최댓값: {}'.format(np.max(train_length)))\n",
    "print('데이터 길이 최솟값: {}'.format(np.min(train_length)))\n",
    "print('데이터 길이 평균값: {:.2f}'.format(np.mean(train_length)))\n",
    "print('데이터 길이 표준편차: {:.2f}'.format(np.std(train_length)))\n",
    "print('데이터 길이 중간값: {}'.format(np.median(train_length)))\n",
    "print('데이터 길이 제1사분위: {}'.format(np.percentile(train_length,25)))\n",
    "print('데이터 길이 제3사분위: {}'.format(np.percentile(train_length,75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 아닌 데이터 모두 제거\n",
    "train_review = [review for review in train_data['발화'] if type(review) is str]\n",
    "train_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불안 0, 분노 1, 기쁨 2\n",
    "\n",
    "print('불안 데이터 갯수: {}'.format(train_data['감정'].value_counts()[0]))\n",
    "print('분노 데이터 갯수: {}'.format(train_data['감정'].value_counts()[1]))\n",
    "print('기쁨 데이터 갯수: {}'.format(train_data['감정'].value_counts()[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "DATA_PATH = 'C:/Users/MJ/Desktop/MyValue/NLP/Model_/data/'\n",
    "train_data = pd.read_csv(DATA_PATH + 'TRAINSET.txt', header = 0, delimiter = '\\t', quoting=3, encoding='cp949')\n",
    "\n",
    "train_data['발화'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 함수 만들기\n",
    "def preprocessing(review, okt, remove_stopwords = False, stop_words = []):\n",
    "  #함수인자설명\n",
    "  # review : 전처리할 텍스트\n",
    "  # okt : okt객체를 반복적으로 생성하지 않고 미리 생성 후 인자로 받음\n",
    "  # remove_stopword : 불용어를 제거할지 여부 선택. 기본값 False\n",
    "  # stop_words : 불용어 사전은 사용자가 직접 입력, 기본값 빈 리스트\n",
    "\n",
    "  #1. 한글 및 공백 제외한 문자 모두 제거\n",
    "  review_text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]','',review)\n",
    "  \n",
    "  #2. okt 객체를 활용하여 형태소 단어로 나눔\n",
    "  word_review = okt.morphs(review_text,stem=True)\n",
    "\n",
    "  if remove_stopwords:\n",
    "    #3. 불용어 제거\n",
    "    word_review = [token for token in word_review if not token in stop_words]\n",
    "  return word_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 텍스트 전처리\n",
    "stop_words = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한']\n",
    "okt = Okt()\n",
    "clean_train_review = []\n",
    "\n",
    "for review in train_data['발화']:\n",
    "  # 데이터가 문자열인 경우만 전처리 진행\n",
    "  if type(review) == str:\n",
    "    clean_train_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words= stop_words))\n",
    "  else:\n",
    "    clean_train_review.append([]) # str이 아닌 행은 빈칸으로 놔두기  \n",
    "\n",
    "clean_train_review[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 리뷰도 동일하게 전처리\n",
    "test_data = pd.read_csv(DATA_PATH + 'TESTCASE.txt', header = 0, delimiter='\\t', quoting=3, encoding='cp949')\n",
    "\n",
    "clean_test_review = []\n",
    "for review in test_data['발화']:\n",
    "  if type(review) == str:\n",
    "    clean_test_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))\n",
    "  else:\n",
    "    clean_test_review.append([])\n",
    "    \n",
    "print(clean_test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정\n",
    "\n",
    "# 인덱스 벡터 변환 후 일정 길이 넘어가거나 모자라는 리뷰 패딩처리\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_review)\n",
    "train_sequences = tokenizer.texts_to_sequences(clean_train_review)\n",
    "test_sequences = tokenizer.texts_to_sequences(clean_test_review)\n",
    "\n",
    "word_vocab = tokenizer.word_index # 단어사전형태\n",
    "MAX_SEQUENCE_LENGTH = 8 # 문장 최대 길이\n",
    "\n",
    "# 학습 데이터\n",
    "train_inputs = pad_sequences(train_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "# 학습 데이터 라벨 벡터화\n",
    "train_labels = np.array(train_data['감정'])\n",
    "\n",
    "# 평가 데이터 \n",
    "test_inputs = pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "# 평가 데이터 라벨 벡터화\n",
    "test_label_data = np.array(test_data['감정'])\n",
    "test_labels = to_categorical(test_label_data, num_classes=3) # 수정할 부분 : 출력 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PATH  = 'C:/Users/MJ/Desktop/MyValue/NLP/Model_/data/' # 경로지정\n",
    "DATA_PATH = 'CLEAN_DATA/' #.npy파일 저장 경로지정\n",
    "TRAIN_INPUT_DATA = 'nsmc_train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'nsmc_train_label.npy'\n",
    "TEST_INPUT_DATA = 'nsmc_test_input.npy'\n",
    "TEST_LABEL_DATA = 'nsmc_test_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "data_configs={}\n",
    "data_configs['vocab'] = word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab) + 1\n",
    "\n",
    "#전처리한 데이터들 파일로저장\n",
    "import os\n",
    "\n",
    "if not os.path.exists(DEFAULT_PATH + DATA_PATH):\n",
    "  os.makedirs(DEFAULT_PATH+DATA_PATH)\n",
    "\n",
    "#전처리 학습데이터 넘파이로 저장\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_INPUT_DATA,'wb'),train_inputs)\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_LABEL_DATA,'wb'),train_labels)\n",
    "#전처리 테스트데이터 넘파이로 저장\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TEST_INPUT_DATA,'wb'),test_inputs)\n",
    "np.save(open(DEFAULT_PATH+DATA_PATH+TEST_LABEL_DATA,'wb'),test_labels)\n",
    "\n",
    "#데이터 사전 json으로 저장\n",
    "json.dump(data_configs,open(DEFAULT_PATH + DATA_PATH + DATA_CONFIGS,'w'),ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 불러오기\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 전처리 데이터 불러오기\n",
    "DATA_PATH = 'C:/Users/MJ/Desktop/MyValue/NLP/Model_/data/CLEAN_DATA/'\n",
    "DATA_OUT = 'C:/Users/MJ/Desktop/MyValue/NLP/Model_/data/DATA_OUT/'\n",
    "INPUT_TRAIN_DATA = 'nsmc_train_input.npy'\n",
    "LABEL_TRAIN_DATA = 'nsmc_train_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "train_input = np.load(open(DATA_PATH + INPUT_TRAIN_DATA,'rb'), allow_pickle=True)\n",
    "train_input = pad_sequences(train_input,maxlen=train_input.shape[1])\n",
    "train_label = np.load(open(DATA_PATH + LABEL_TRAIN_DATA,'rb'), allow_pickle=True)\n",
    "prepro_configs = json.load(open(DATA_PATH+DATA_CONFIGS,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn_classifier_kr'\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 10\n",
    "VALID_SPLIT = 0.1\n",
    "MAX_LEN = train_input.shape[1]\n",
    "\n",
    "kargs={'model_name': model_name, 'vocab_size':prepro_configs['vocab_size'],'embbeding_size':128, 'num_filters':100,'dropout_rate':0.5, 'hidden_dimension':250,'output_dimension':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정\n",
    "\n",
    "class CNNClassifier(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, **kargs):\n",
    "        super(CNNClassifier, self).__init__(name=kargs['model_name'])\n",
    "        self.embedding = layers.Embedding(input_dim=kargs['vocab_size'], output_dim=kargs['embbeding_size'])\n",
    "        self.conv_list = [layers.Conv1D(filters=kargs['num_filters'], kernel_size=kernel_size, padding='valid',activation = tf.keras.activations.relu,\n",
    "                                        kernel_constraint = tf.keras.constraints.MaxNorm(max_value=3)) for kernel_size in [3,4,5]]\n",
    "        self.pooling = layers.GlobalMaxPooling1D()\n",
    "        self.dropout = layers.Dropout(kargs['dropout_rate'])\n",
    "        self.fc1 = layers.Dense(units=kargs['hidden_dimension'],\n",
    "                                activation = tf.keras.activations.relu,\n",
    "                                kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "        self.fc2 = layers.Dense(units=3,  # 수정할 부분 : 출력 차원 변경\n",
    "                                activation=tf.keras.activations.softmax,  # 활성화 함수 변경\n",
    "                                kernel_constraint= tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis = 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정\n",
    "\n",
    "from keras.models import save_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = CNNClassifier(**kargs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(name='accuracy')])\n",
    "\n",
    "train_label = to_categorical(train_label, num_classes=3) # 수정할 부분\n",
    "\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
    "checkpoint_path = DATA_OUT + model_name + '\\weights.h5'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "\n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "history = model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
    "\n",
    "save_model(model, 'C:/Users/MJ/Desktop/MyValue/NLP/Model_/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEST_DATA = 'nsmc_test_input.npy'\n",
    "LABEL_TEST_DATA = 'nsmc_test_label.npy'\n",
    "SAVE_FILE_NM = 'weights.h5'\n",
    "\n",
    "test_input = np.load(open(DATA_PATH+INPUT_TEST_DATA,'rb'))\n",
    "test_input = pad_sequences(test_input,maxlen=test_input.shape[1])\n",
    "test_label_data = np.load(open(DATA_PATH + LABEL_TEST_DATA, 'rb'))\n",
    "\n",
    "model.load_weights('C:/Users/MJ/Desktop/MyValue/NLP/Model_/data/DATA_OUT/cnn_classifier_kr/weights.h5')\n",
    "model.evaluate(test_input, test_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "okt = Okt()\n",
    "tokenizer  = Tokenizer()\n",
    "\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "prepro_configs = json.load(open('C:/Users/MJ/Desktop/MyValue/NLP/Model_/data/CLEAN_DATA/'+DATA_CONFIGS,'r'))\n",
    "prepro_configs['vocab'] = word_vocab\n",
    "\n",
    "tokenizer.fit_on_texts(word_vocab)\n",
    "\n",
    "MAX_LENGTH = 8 # 문장최대길이\n",
    "\n",
    "sentence = input('감성분석할 문장을 입력해 주세요.: ')\n",
    "sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\\\s ]','', sentence)\n",
    "stopwords = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한'] # 불용어 추가할 것이 있으면 추가\n",
    "sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "sentence = [word for word in sentence if not word in stopwords] # 불용어 제거\n",
    "vector  = tokenizer.texts_to_sequences(sentence)\n",
    "pad_new = pad_sequences(vector, maxlen = MAX_LENGTH) # 패딩\n",
    "\n",
    "print(sentence)\n",
    "\n",
    "model.load_weights('C:/Users/MJ/Desktop/MyValue/NLP/Model_/data/DATA_OUT/cnn_classifier_kr/weights.h5') # 모델 불러오기\n",
    "predictions = model.predict(pad_new)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 주어진 결과값\n",
    "emotions = np.array(predictions)\n",
    "\n",
    "# 감정 레이블\n",
    "emotion_labels = ['불안', '분노', '기쁨']\n",
    "\n",
    "# 감정 요약 함수\n",
    "def summarize_emotion(emotions, threshold=0.35):\n",
    "    summary = []\n",
    "    for e in emotions:\n",
    "        dominant_emotion = np.argmax(e)  # 가장 높은 점수의 감정 인덱스\n",
    "        if e[dominant_emotion] > threshold:  # 임계값 이상의 점수만을 고려\n",
    "            summary.append(emotion_labels[dominant_emotion])\n",
    "    return summary\n",
    "\n",
    "# 전체적인 기분 요약\n",
    "overall_summary = summarize_emotion(emotions)\n",
    "print(overall_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 실제 감정 데이터와 레이블\n",
    "emotions = np.array(predictions)\n",
    "emotion_labels = ['Anxiety', 'Anger', 'Pleasure']\n",
    "\n",
    "# 실제 요약된 감정 데이터\n",
    "overall_summary = summarize_emotion(emotions)\n",
    "\n",
    "# 요약된 감정 빈도 계산\n",
    "emotion_counts = {emotion: overall_summary.count(emotion) for emotion in emotion_labels}\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(emotion_labels, [emotion_counts[emotion] for emotion in emotion_labels], color='skyblue')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Emotion Graph')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화까지\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "okt = Okt()\n",
    "tokenizer  = Tokenizer()\n",
    "\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "prepro_configs = json.load(open('C:/Users/MJ/Desktop/MyValue/NLP/Model_/data/CLEAN_DATA/'+DATA_CONFIGS,'r'))\n",
    "prepro_configs['vocab'] = word_vocab\n",
    "\n",
    "tokenizer.fit_on_texts(word_vocab)\n",
    "\n",
    "MAX_LENGTH = 8 # 문장최대길이\n",
    "\n",
    "sentence = input('감성분석할 문장을 입력해 주세요.: ')\n",
    "sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\\\s ]','', sentence)\n",
    "stopwords = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한'] # 불용어 추가할 것이 있으면 추가\n",
    "sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "sentence = [word for word in sentence if not word in stopwords] # 불용어 제거\n",
    "vector  = tokenizer.texts_to_sequences(sentence)\n",
    "pad_new = pad_sequences(vector, maxlen = MAX_LENGTH) # 패딩\n",
    "\n",
    "model.load_weights('C:/Users/MJ/Desktop/MyValue/NLP/Model_/data/DATA_OUT/cnn_classifier_kr/weights.h5') # 모델 불러오기\n",
    "predictions = model.predict(pad_new)\n",
    "\n",
    "print(sentence)\n",
    "print(predictions)\n",
    "\n",
    "# 주어진 결과값\n",
    "emotions = np.array(predictions)\n",
    "\n",
    "# 감정 레이블\n",
    "emotion_labels = ['불안', '분노', '기쁨']\n",
    "\n",
    "# 감정 요약 함수\n",
    "def summarize_emotion(emotions, threshold=0.35): # 임계값 설정\n",
    "    summary = []\n",
    "    for e in emotions:\n",
    "        dominant_emotion = np.argmax(e)  # 가장 높은 점수의 감정 인덱스\n",
    "        if e[dominant_emotion] > threshold:  # 임계값 이상의 점수만을 고려\n",
    "            summary.append(emotion_labels[dominant_emotion])\n",
    "    return summary\n",
    "\n",
    "# 전체적인 기분 요약\n",
    "overall_summary = summarize_emotion(emotions)\n",
    "print(overall_summary)\n",
    "\n",
    "# 실제 감정 데이터와 레이블\n",
    "emotions = np.array(predictions)\n",
    "emotion_labels = ['Anxiety', 'Anger', 'Pleasure']\n",
    "\n",
    "# 실제 요약된 감정 데이터\n",
    "overall_summary = summarize_emotion(emotions)\n",
    "\n",
    "# 요약된 감정 빈도 계산\n",
    "emotion_counts = {emotion: overall_summary.count(emotion) for emotion in emotion_labels}\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(emotion_labels, [emotion_counts[emotion] for emotion in emotion_labels], color='skyblue')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Emotion Graph')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
